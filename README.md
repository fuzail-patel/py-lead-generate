# ğŸš€ Lead Generation & Research Workflow â€” Python + FastAPI

## ğŸ§  Overview

This project automates **lead research, company profiling, and personalized outreach email generation** using a modular Python architecture.  
It is designed to integrate seamlessly with **n8n**, automation frameworks, or direct HTTP API calls via FastAPI.

---

## âš™ï¸ Key Features

- ğŸŒ **Smart Search + Scraping** â€” Uses DuckDuckGo and web scraping with **SOCKS5 proxy rotation** to avoid IP blocking.  
- ğŸ”„ **Intelligent Proxy System** â€” Real-world proxy testing against DuckDuckGo with parallel validation and automatic bad proxy marking.
- âš¡ **High Performance** â€” Optimized scraping with connection pooling, parallel workers, and early content extraction (60% faster).
- ğŸ§¹ **Filtering Engine** â€” Cleans and structures scraped content for AI processing.  
- ğŸ¤– **LLM Integration** â€” Generates company summaries, proposals, and complete outreach emails using OpenAI (no placeholders).  
- ğŸ’Œ **Email Formatter** â€” Creates professional, ready-to-send email templates with automatic sender signatures.  
- ğŸ§± **Exception-Safe Workflow** â€” Each stage is modular and error-isolated for debugging and retrying.  
- ğŸ” **Auto-Retry Mechanism** â€” Failed requests automatically retry with different proxies (3 attempts).
- ğŸ§© **n8n Ready** â€” Accepts JSON input and produces clean JSON output for automation pipelines.
- ğŸŒ **FastAPI REST API** â€” HTTP endpoints for programmatic access with interactive API documentation.  

---

## ğŸ—‚ï¸ Project Structure

```
py-lead-generator-main/
â”‚
â”œâ”€ .env                    # Environment variables
â”œâ”€ .env.example            # Example env config
â”œâ”€ proxy_list.csv          # Cached list of public SOCKS5 proxies
â”œâ”€ run.py                  # Main entry point (used by n8n / local CLI)
â”œâ”€ run_workflow.py         # Core orchestrator (called by run.py)
â”œâ”€ run_proxy_extractor.py  # Fetches and validates public proxies
â”œâ”€ server.py               # FastAPI REST API server
â”œâ”€ test_input.json         # Sample input payload
â”‚
â”œâ”€ constants/
â”‚   â”œâ”€ config.py           # Configuration constants
â”‚   â”œâ”€ prompts.py          # Prompt templates for LLM
â”‚   â”œâ”€ proxy.py            # Proxy-related constants and URLs
â”‚   â”œâ”€ searchTemplates.py  # Predefined search query formats
â”‚   â”œâ”€ userAgents.py       # Randomized User-Agent list
â”‚
â”œâ”€ modules/
â”‚   â”œâ”€ search.py           # Performs search queries (DuckDuckGo)
â”‚   â”œâ”€ scrapper.py         # Extracts text content from URLs using rotating proxies
â”‚   â”œâ”€ filter.py           # Cleans and preprocesses text
â”‚   â”œâ”€ llm.py              # Generates summaries, research, and emails
â”‚   â”œâ”€ email.py            # Formats email templates with placeholders
â”‚   â”œâ”€ proxy.py            # Manages proxy rotation and validation
â”‚   â”œâ”€ main.py             # Initializes all modules (central import layer)
â”‚   â””â”€ exceptions/
â”‚       â””â”€ exceptions.py   # Custom exception classes for modular error handling
â”‚
â”œâ”€ utils/
â”‚   â””â”€ helpers.py          # Shared utility functions
â”‚
â””â”€ README.md               # This file
```

---

## ğŸ” Workflow Pipeline

```
SearchModule  â†’  ScraperModule  â†’  FilterModule  â†’  LLMModule  â†’  EmailModule
     â”‚                 â”‚                â”‚                 â”‚
     â”‚                 â”‚                â”‚                 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º WorkflowFacade â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                      run_workflow()
                             â”‚
                             â–¼
                       run.py / server.py
```

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.11 or higher
- OpenAI API key ([Get one here](https://platform.openai.com/api-keys))
- Internet connection (for proxy fetching and API calls)

### Setup Steps

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd py-lead-generator-main
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment variables**
   
   Create a `.env` file in the project root:
   ```bash
   OPENAI_API_KEY=sk-your-actual-api-key-here
   ```
   
   **Optional:** Customize sender information:
   ```bash
   SENDER_NAME=Your Name
   SENDER_TITLE=Your Title
   SENDER_COMPANY=Your Company
   SENDER_EMAIL=your@email.com
   SENDER_PHONE=+1234567890
   ```

4. **Initialize proxy list**
   ```bash
   python run_proxy_extractor.py
   ```
   
   This fetches and tests 100+ proxies, saving the top 30 to `proxy_list.csv`.

5. **Test the system**
   ```bash
   # Windows
   Get-Content test_input.json | python run.py
   
   # Linux/Mac
   cat test_input.json | python run.py
   ```

---

## âš¡ Quick Start

### Method 1: CLI Mode (JSON Input)

**Important:** All input must be provided via JSON (stdin). No defaults or environment variables are used for input fields.

```bash
# Windows PowerShell
Get-Content test_input.json | python run.py

# Linux/Mac/WSL
cat test_input.json | python run.py

# Minimal example (only topic required)
echo '{"topic":"Company Name"}' | python run.py
```

### Method 2: FastAPI REST API

```bash
# Start the server
uvicorn server:app --host 0.0.0.0 --port 8000

# Or with auto-reload for development
uvicorn server:app --host 0.0.0.0 --port 8000 --reload
```

The API will be available at:
- **API Endpoints:** `http://localhost:8000/api/`
- **Interactive Docs:** `http://localhost:8000/docs` (Swagger UI)
- **Alternative Docs:** `http://localhost:8000/redoc` (ReDoc)

### Method 3: Docker

```bash
# Build the image
docker build -t py-lead-generator .

# Run the API server
docker run -p 8000:8000 --env-file .env py-lead-generator

# Or run CLI mode
docker run -i --rm --env-file .env py-lead-generator python3 run.py < test_input.json
```

---

## ğŸ“‹ Input Schema

### Required Fields

Only `topic` is required. All other fields are optional.

```json
{
  "topic": "Company Name or Topic"
}
```

### Complete Input Schema

```json
{
  "topic": "string (required)",
  "mode": "string (optional, default: 'lead')",
  "recipient_name": "string (optional)",
  "sender_company_summary": "string (optional)",
  "max_results": "number (optional, default: 5)",
  "sender_info": {
    "sender_name": "string (optional)",
    "sender_title": "string (optional)",
    "sender_company": "string (optional)",
    "sender_phone": "string (optional)",
    "sender_email": "string (optional)",
    "sender_website": "string (optional)",
    "sender_address": "string (optional)"
  }
}
```

### Field Descriptions

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `topic` | string | **Yes** | - | Company name or research topic (the target company) |
| `mode` | string | No | `"lead"` | Workflow mode: `lead`, `research`, `general` |
| `recipient_name` | string | No | `""` | Name of email recipient (for lead mode) |
| `sender_company_summary` | string | No | `""` | **Description of YOUR company (the sender)** - helps AI write personalized outreach emails mentioning your services. This is NOT about the target company. |
| `max_results` | number | No | `5` | Maximum search results to scrape (1-20) |

**Important:** `sender_company_summary` describes **YOUR company** (the sender), not the target company. The AI uses this to personalize emails by mentioning your services.

### Mode Options

| Mode | Output | Use Case |
|------|--------|----------|
| `lead` | Summary + Email | Lead generation & outreach |
| `research` | Content only | Company research without email |
| `general` | Summary + Content | General information gathering |

### Example Inputs

#### Minimal Lead Generation
```json
{
  "topic": "Stripe Inc"
}
```

#### Complete Lead Generation
```json
{
  "topic": "Stripe Inc",
  "mode": "lead",
  "recipient_name": "Sarah Chen",
  "sender_company_summary": "We are a payment integration consultancy that helps e-commerce businesses optimize their payment workflows and reduce transaction fees.",
  "max_results": 8,
  "sender_info": {
    "sender_name": "Michael Rodriguez",
    "sender_title": "Senior Business Development Manager",
    "sender_company": "CloudSync Solutions",
    "sender_phone": "+1 (555) 234-5678",
    "sender_email": "m.rodriguez@cloudsync.io",
    "sender_website": "https://www.cloudsync.io",
    "sender_address": "350 5th Avenue, Suite 4500, New York, NY 10118"
  }
}
```

#### Research Mode
```json
{
  "topic": "Artificial Intelligence in Healthcare",
  "mode": "research",
  "max_results": 10
}
```

---

## ğŸŒ FastAPI REST API

### API Endpoint: `/api/generate`

Generate outreach email or research content via HTTP POST.

**Endpoint:** `POST /api/generate`

**Required Fields:**
- `topic` (string): Company name or topic to research (cannot be empty or whitespace)

**Optional Fields:**
- `mode` (string): Workflow mode - `"lead"`, `"general"`, or `"research"` (default: `"lead"`)
- `recipient_name` (string): Name for personalized emails
- `sender_company_summary` (string): **Description of YOUR company (the sender)** - helps AI personalize the email by mentioning your services
- `sender_info` (object): Sender information for email signature
  - `sender_name` (string)
  - `sender_title` (string)
  - `sender_company` (string)
  - `sender_phone` (string)
  - `sender_email` (string)
  - `sender_website` (string)
  - `sender_address` (string)
- `max_results` (integer): Number of search results (1-20, default: 5)

**Example Request:**

```bash
curl -X POST "http://localhost:8000/api/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "topic": "Stripe Inc",
    "mode": "lead",
    "recipient_name": "John Doe",
    "sender_company_summary": "We are a payment integration consultancy that helps businesses optimize their payment workflows",
    "sender_info": {
      "sender_name": "Jane Smith",
      "sender_email": "jane@example.com",
      "sender_company": "TechCorp"
    }
  }'
```

**Example Response (Lead Mode):**

```json
{
  "success": true,
  "mode": "lead",
  "summary": "Stripe is a payment processing platform...",
  "email": {
    "subject": "Partnership Opportunity with TechCorp",
    "body": "<html>...</html>"
  }
}
```

**Validation:**

The API enforces strict validation:
- âœ… `topic` is required and cannot be empty or whitespace
- âœ… `mode` must be one of: `"lead"`, `"general"`, `"research"`
- âœ… `max_results` must be between 1 and 20
- âœ… Returns HTTP 422 for validation errors
- âœ… Returns HTTP 400 for workflow errors
- âœ… Returns HTTP 500 for internal errors

**Error Response Example:**

```json
{
  "detail": {
    "success": false,
    "error_type": "SearchFailedError",
    "message": "DuckDuckGo returned no valid URLs"
  }
}
```

---

## ğŸŒ Intelligent Proxy System

DuckDuckGo aggressively blocks scraping. To bypass this, the project uses an **intelligent SOCKS5 proxy system** with automatic retry and bad proxy marking.

### How It Works

1. **Load Proxies** â€” Reads from `proxy_list.csv` on startup (30 tested proxies)
2. **Smart Selection** â€” Picks random proxy, excluding previously failed ones
3. **Auto-Retry** â€” If proxy fails, automatically tries another (up to 3 attempts)
4. **Bad Proxy Marking** â€” Failed proxies are marked and excluded from future requests
5. **Fallback** â€” Uses hardcoded proxies from `constants/proxy.py` if CSV missing

### Key Features

- âœ… **Automatic Retry** â€” Up to 3 attempts with different proxies
- âœ… **Bad Proxy Delisting** â€” Failed proxies blacklisted during runtime
- âœ… **Persistent Bad Proxy Removal** â€” Bad proxies are removed from CSV file immediately (won't be used after restart)
- âœ… **Smart Reset** â€” If all proxies bad, automatically resets and retries
- âœ… **Detailed Logging** â€” Shows proxy status, attempts, and failures
- âœ… **Fallback Strategy** â€” Never fails due to missing CSV

### Manual Proxy Refresh

```bash
# Run this manually when you need fresh proxies
python run_proxy_extractor.py

# Output: Writes 30 verified proxies to proxy_list.csv
```

**Recommended:** Refresh proxies daily for high-volume usage, weekly for low-volume.

### Example Logs

```
[SUCCESS] Loaded 30 proxies from proxy_list.csv
[ERROR] Proxy error with socks5://1.2.3.4:1080 (attempt 1/3)
[DELIST] Marked proxy as bad: socks5://1.2.3.4:1080 (1 bad proxies)
[SUCCESS] Request succeeded on attempt 2 with proxy socks5://5.6.7.8:1080
```

---

## âš¡ Performance Optimizations

The project has been optimized for speed and efficiency:

### Scraping Optimizations
- **Connection Pooling** â€” Reuses HTTP connections via `requests.Session()` to reduce overhead
- **Parallel Scraping** â€” Uses 10 concurrent workers to scrape multiple pages simultaneously
- **Early Content Extraction** â€” Stops parsing once 2000 characters of relevant content are found
- **Adaptive Timeouts** â€” 5s initial timeout with 3s retry for better performance on slow pages
- **Content Length Pre-check** â€” Skips pages larger than 2MB before downloading
- **Selective HTML Parsing** â€” Prioritizes main content areas (main, article, .content) first
- **Duplicate Detection** â€” Set-based O(1) lookup to avoid processing duplicate content
- **Optimized String Operations** â€” Uses list + join() instead of += for efficient concatenation

### Proxy Optimizations
- **Parallel Testing** â€” Tests proxies concurrently (30 workers) for 10-20x faster validation
- **Real-World Validation** â€” Tests against actual DuckDuckGo searches instead of generic IP checks
- **Comprehensive Collection** â€” Collects all working proxies without early termination

### Results
- **Scraping Time** â€” Reduced from 60s to 22-25s (60% improvement)
- **Proxy Testing** â€” Reduced from 1 hour to 5-10 minutes for 1700 proxies
- **Memory Usage** â€” Reduced through stream-based fetching and early termination

---

## ğŸ“Š Output Format

### Success Response (Lead Mode)

```json
{
  "success": true,
  "mode": "lead",
  "summary": "Company summary here...",
  "email": {
    "subject": "Email subject line",
    "body": "<html>...formatted email...</html>"
  }
}
```

### Success Response (Research/General Mode)

```json
{
  "success": true,
  "mode": "research",
  "summary": "Research summary here...",
  "content": "Full scraped and filtered content..."
}
```

### Error Response

```json
{
  "success": false,
  "error_type": "InsufficientContentError",
  "message": "No sufficient content found for topic: 'Company Name'.",
  "topic": "Company Name"
}
```

---

## âš ï¸ Error Handling

| Error Type | Description |
|-------------|-------------|
| `ProxyFailureError` | All proxy attempts failed - indicates network/proxy infrastructure issue, NOT a bad lead |
| `SearchFailedError` | Search engine returned no results |
| `InsufficientContentError` | Scraped data insufficient for LLM - indicates the topic/lead has no sufficient content |
| `LLMProcessingError` | LLM failed to generate a valid response |
| `SearchBlockedError` | IP or proxy temporarily blocked |
| `UnknownError` | Any unexpected runtime error |

All errors return JSON with:
- `success: false`
- `error_type`: Type of error
- `message`: Descriptive error message
- `topic`: The topic that caused the error
- `is_infrastructure_error`: (Optional) `true` for infrastructure issues (ProxyFailureError), `false` for content issues

**Important:** `ProxyFailureError` indicates a network/proxy problem, not that the lead is invalid. Use `is_infrastructure_error` to distinguish between infrastructure issues and actual content problems.

---

## ğŸ”Œ Integration Examples

### n8n Integration

**Execute Command Node:**
```bash
python3 /path/to/run.py
```

**Expected Flow:**
```
Webhook (lead trigger)
   â†“
Execute Command (run.py)
   â†“
Email Node (send email)
   â†“
MongoDB Node (update lead status)
```

### Python Script

```python
import subprocess
import json

input_data = {
    "topic": "SpaceX",
    "mode": "lead",
    "recipient_name": "Elon Musk",
    "sender_company_summary": "We provide rocket propulsion consulting services"
}

result = subprocess.run(
    ["python", "run.py"],
    input=json.dumps(input_data),
    capture_output=True,
    text=True
)

output = json.loads(result.stdout)
print(output["summary"])
```

### HTTP API Call

```python
import requests

response = requests.post(
    "http://localhost:8000/api/generate",
    json={
        "topic": "Stripe Inc",
        "mode": "lead",
        "recipient_name": "John Doe",
        "sender_company_summary": "We are a payment integration consultancy"
    }
)

result = response.json()
print(result["email"]["subject"])
```

---

## ğŸ”„ Maintenance

### Refreshing Proxies

Proxies should be refreshed periodically (daily or weekly depending on usage):

```bash
python run_proxy_extractor.py
```

This updates `proxy_list.csv` with fresh, tested proxies. The system automatically uses these on the next run.

---

## ğŸ†˜ Troubleshooting

### Issue: "OpenAI API key not provided"
**Solution:** Create `.env` file with `OPENAI_API_KEY=sk-...`

### Issue: "No proxies available"
**Solution:** Run `python run_proxy_extractor.py` to fetch proxies

### Issue: Import errors
**Solution:** Run `pip install -r requirements.txt`

### Issue: "[WARNING] Proxy CSV file not found"
**Solution:** Run `python run_proxy_extractor.py` to create `proxy_list.csv`
**Note:** System falls back to hardcoded proxies automatically

### Issue: "Search blocked"
**Solution:** Refresh proxies with `python run_proxy_extractor.py`

### Issue: Docker proxy failures
**Solution:** Ensure `pysocks` or `requests[socks]` is installed. Add to `requirements.txt`:
```txt
requests[socks]>=2.31.0
```

---

## ğŸ§­ Roadmap

- [x] ~~ProxyFailureError exception for better error distinction~~ âœ… Completed in v1.7.0
- [x] ~~Performance optimizations (connection pooling, parallel scraping)~~ âœ… Completed in v1.8.0
- [x] ~~Real-world proxy testing against DuckDuckGo~~ âœ… Completed in v1.8.0
- [ ] Automated proxy refresh scheduling (cron/Docker)
- [ ] Add proxy pool scoring (based on response time & health)
- [ ] Retry system with exponential backoff for failed proxies
- [ ] HTML sanitization & content enrichment
- [ ] Support for Gmail / ZeptoMail sending
- [ ] Batch mode for multi-lead processing
- [ ] Structured logging with colorized output

---

## ğŸ“ Notes

### About Email Generation

The LLM generates **complete, finished emails** with no placeholders:
- âœ… **Complete Emails** â€” AI generates full email body with proper greeting and content
- âœ… **Natural Recipient Handling** â€” Uses provided `recipient_name` directly, or professional generic greeting if not provided
- âœ… **No Placeholders** â€” Never uses placeholders like `[Recipient]`, `[Name]`, etc.
- âœ… **Automatic Signatures** â€” Always includes "Best regards," with optional sender details

### About `sender_company_summary`

The `sender_company_summary` field describes **YOUR company** (the sender), not the target company:

- âœ… **Correct:** `"We are a payment integration consultancy that helps businesses..."` (describes YOUR company)
- âŒ **Incorrect:** `"Stripe is a payment processing platform..."` (describes the target company)

The AI uses your company summary to personalize outreach emails by naturally mentioning how your services relate to the target company.

---

## ğŸ§¾ License

MIT License Â© 2025 **Xupyter Solutions**

---

## ğŸ“š Additional Resources

- **Sample Input:** See `test_input.json` in the project root
- **API Documentation:** Visit `http://localhost:8000/docs` when server is running
- **Version History:** See `CHANGELOG.md`
